{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73842fac",
   "metadata": {},
   "source": [
    "\n",
    "# MCP Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321d1258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install mcp, langgraph, and langchain if needed (uncomment when running locally)\n",
    "# !pip install langchain-mcp-adapters langgraph>=0.2.0 langchain-openai mcp\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "import os\n",
    "## SET YOUR OPENAI_API_KEY HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd5abc130d8cb897",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T06:19:30.841226Z",
     "start_time": "2025-09-09T06:19:30.836055Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "\n",
    "\n",
    "# Helper function to parse and display messages and tool calls\n",
    "def parse_output(result_dict):\n",
    "    \"\"\"\n",
    "    Parse the output from demo_multi_server() and display the conversation flow\n",
    "    including user input, tool calls, tool responses, and final agent response.\n",
    "\n",
    "    Args:\n",
    "        result_dict: The result dictionary containing 'messages' list\n",
    "    \"\"\"\n",
    "    messages = result_dict.get('messages', [])\n",
    "\n",
    "    print(\"=== CONVERSATION FLOW ===\")\n",
    "    print()\n",
    "\n",
    "    for i, message in enumerate(messages):\n",
    "        print(f\"Step {i + 1}:\")\n",
    "\n",
    "        if isinstance(message, HumanMessage):\n",
    "            print(f\"  USER INPUT: {message.content}\")\n",
    "\n",
    "        elif isinstance(message, AIMessage):\n",
    "            if hasattr(message, 'tool_calls') and message.tool_calls:\n",
    "                print(f\"  AGENT TOOL CALLS:\")\n",
    "                for tool_call in message.tool_calls:\n",
    "                    print(f\"    - Tool: {tool_call['name']}\")\n",
    "                    print(f\"      Arguments: {tool_call['args']}\")\n",
    "                    print(f\"      Call ID: {tool_call['id']}\")\n",
    "            else:\n",
    "                print(f\"  AGENT RESPONSE: {message.content}\")\n",
    "\n",
    "        elif isinstance(message, ToolMessage):\n",
    "            print(f\"  TOOL OUTPUT:\")\n",
    "            print(f\"    - Tool: {message.name}\")\n",
    "            print(f\"    - Result: {message.content}\")\n",
    "            print(f\"    - Call ID: {message.tool_call_id}\")\n",
    "\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "567f56f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T06:19:55.954802Z",
     "start_time": "2025-09-09T06:19:52.033729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tools: ['read_file', 'read_text_file', 'read_media_file', 'read_multiple_files', 'write_file', 'edit_file', 'create_directory', 'list_directory', 'list_directory_with_sizes', 'directory_tree', 'move_file', 'search_files', 'get_file_info', 'list_allowed_directories'] \n",
      "\n",
      "\n",
      "=== CONVERSATION FLOW ===\n",
      "\n",
      "Step 1:\n",
      "  USER INPUT: List all files in ExampleFolder directory\n",
      "\n",
      "Step 2:\n",
      "  AGENT TOOL CALLS:\n",
      "    - Tool: list_directory\n",
      "      Arguments: {'path': 'ExampleFolder'}\n",
      "      Call ID: call_wKnqw21atmVfK4Z2Ve4t4XcY\n",
      "\n",
      "Step 3:\n",
      "  TOOL OUTPUT:\n",
      "    - Tool: list_directory\n",
      "    - Result: [FILE] file1.txt\n",
      "[FILE] file2.txt\n",
      "[FILE] file3.txt\n",
      "    - Call ID: call_wKnqw21atmVfK4Z2Ve4t4XcY\n",
      "\n",
      "Step 4:\n",
      "  AGENT RESPONSE: Here are the files in the \"ExampleFolder\" directory:\n",
      "\n",
      "1. file1.txt\n",
      "2. file2.txt\n",
      "3. file3.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "async def demo_single_server():\n",
    "    \"\"\"Demonstrate connecting to a filesystem MCP server using MultiServerMCPClient.\n",
    "\n",
    "    This example launches the official filesystem server from the Model Context Protocol\n",
    "    project using `npx mcp-server-filesystem`. Update the root path as needed.\n",
    "\n",
    "    \"\"\"\n",
    "    model = ChatOpenAI(model='gpt-4', temperature=0)\n",
    "    # Define connection configuration for the filesystem server\n",
    "    connections = {\n",
    "        \"filesystem\": {\n",
    "            \"command\": \"npx\",\n",
    "            \"args\": [\n",
    "                \"-y\",\n",
    "                \"@modelcontextprotocol/server-filesystem\",\n",
    "                \"./ExampleFolder\",  # Specify directories that the agent is allowed to access\n",
    "                # Add more directories as needed\n",
    "            ],\n",
    "            \"transport\": \"stdio\",\n",
    "        }\n",
    "    }\n",
    "    client = MultiServerMCPClient(connections)\n",
    "\n",
    "    tools = await client.get_tools(server_name='filesystem')\n",
    "    print('Available tools:', [t.name for t in tools], '\\n\\n')\n",
    "\n",
    "    agent = create_react_agent(model, tools)\n",
    "\n",
    "    # Example query: list files in the root directory\n",
    "    result = await agent.ainvoke({\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"List all files in ExampleFolder directory\"}\n",
    "        ]\n",
    "    })\n",
    "\n",
    "\n",
    "    parse_output(result)\n",
    "\n",
    "\n",
    "\n",
    "# await demo_single_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2407e924",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T06:20:12.641908Z",
     "start_time": "2025-09-09T06:20:06.455123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tools loaded: 16\n",
      "=== CONVERSATION FLOW ===\n",
      "\n",
      "Step 1:\n",
      "  USER INPUT: what's (3 + 5) x 12?\n",
      "\n",
      "Step 2:\n",
      "  AGENT TOOL CALLS:\n",
      "    - Tool: add\n",
      "      Arguments: {'a': 3, 'b': 5}\n",
      "      Call ID: call_OCkwo2ZoYzkTZE6RlEqri7Uk\n",
      "\n",
      "Step 3:\n",
      "  TOOL OUTPUT:\n",
      "    - Tool: add\n",
      "    - Result: 8\n",
      "    - Call ID: call_OCkwo2ZoYzkTZE6RlEqri7Uk\n",
      "\n",
      "Step 4:\n",
      "  AGENT TOOL CALLS:\n",
      "    - Tool: multiply\n",
      "      Arguments: {'a': 8, 'b': 12}\n",
      "      Call ID: call_h6otA1S08g8mXFJ8jkXtHk7g\n",
      "\n",
      "Step 5:\n",
      "  TOOL OUTPUT:\n",
      "    - Tool: multiply\n",
      "    - Result: 96\n",
      "    - Call ID: call_h6otA1S08g8mXFJ8jkXtHk7g\n",
      "\n",
      "Step 6:\n",
      "  AGENT RESPONSE: The result of (3 + 5) x 12 is 96.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "async def demo_multi_server():\n",
    "    \"\"\"Demonstrate connecting to multiple MCP servers using MultiServerMCPClient.\n",
    "\n",
    "    This example combines the filesystem server (run via npx) and a local math server\n",
    "    implemented as a Python script. Update the command paths and root directory\n",
    "    to match your environment.\n",
    "\n",
    "    \"\"\"\n",
    "    model = ChatOpenAI(model='gpt-4', temperature=0)\n",
    "    # Define connection configurations for two servers\n",
    "    connections = {\n",
    "        \"filesystem\": {\n",
    "            \"command\": \"npx\",\n",
    "            \"args\": [\n",
    "                \"-y\",\n",
    "                \"@modelcontextprotocol/server-filesystem\",\n",
    "                \"./ExampleFolder\",\n",
    "                # Add more directories as needed\n",
    "            ],\n",
    "            \"transport\": \"stdio\",\n",
    "        },\n",
    "        'math': {\n",
    "            'command': 'python',\n",
    "            'args': ['./math_server.py'],  # Update with your math server path\n",
    "            'transport': 'stdio',\n",
    "        },\n",
    "    }\n",
    "    client = MultiServerMCPClient(connections)\n",
    "    tools = await client.get_tools()  # load tools from both servers\n",
    "    print('Total tools loaded:', len(tools))\n",
    "    agent = create_react_agent(model, tools)\n",
    "\n",
    "    # Example: ask a math question\n",
    "    math_result = await agent.ainvoke({\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"what's (3 + 5) x 12?\"}\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    parse_output(math_result)\n",
    "\n",
    "# await demo_multi_server()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
