{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCP Exercise: Building a Multi-Server Agent\n",
    "\n",
    "In this exercise, you'll build a LangGraph agent that integrates both filesystem and GitHub MCP servers to automate fetching repository issues and saving summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install mcp, langgraph, and langchain if needed (uncomment when running locally)\n",
    "# !pip install langchain-mcp-adapters langgraph>=0.2.0 langchain-openai mcp\n",
    "\n",
    "import asyncio\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.prebuilt import ToolNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "## SET YOUR OPENAI_API_KEY HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Setup\n",
    "\n",
    "Your task is to create a LangGraph agent that can:\n",
    "1. Connect to both filesystem and GitHub MCP servers\n",
    "2. Fetch the latest issue from a GitHub repository\n",
    "3. Summarize the issue\n",
    "4. Save the summary to a file using the filesystem server\n",
    "\n",
    "Follow the steps below to complete the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def setup_graph() -> StateGraph:\n",
    "    \"\"\"\n",
    "    Create and return a compiled LangGraph that integrates MCP tools.\n",
    "\n",
    "    TODO: Your task is to configure the MCP server connections only.\n",
    "    The LangGraph setup is already implemented for you.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Define MCP server connections\n",
    "    # Configure both filesystem and GitHub server connections\n",
    "    connections = {\n",
    "        # Add filesystem server configuration here\n",
    "        # Add GitHub server configuration here\n",
    "    }\n",
    "\n",
    "    # Create MCP client and get tools\n",
    "    client = MultiServerMCPClient(connections)\n",
    "    tools = await client.get_tools()\n",
    "\n",
    "    # Initialize ChatOpenAI model and bind tools\n",
    "    model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "    model_with_tools = model.bind_tools(tools)\n",
    "\n",
    "    # Create ToolNode for executing tool calls\n",
    "    tool_node = ToolNode(tools)\n",
    "\n",
    "    # Define transition function for graph routing\n",
    "    def should_continue(state: MessagesState):\n",
    "        last = state[\"messages\"][-1]\n",
    "        if last.tool_calls:\n",
    "            return \"tools\"\n",
    "        return END\n",
    "\n",
    "    # Define model invocation node\n",
    "    async def call_model(state: MessagesState):\n",
    "        messages = state[\"messages\"]\n",
    "        response = await model_with_tools.ainvoke(messages)\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    # Build the state graph\n",
    "    builder = StateGraph(MessagesState)\n",
    "    builder.add_node(\"call_model\", call_model)\n",
    "    builder.add_node(\"tools\", tool_node)\n",
    "    builder.add_edge(START, \"call_model\")\n",
    "    builder.add_conditional_edges(\"call_model\", should_continue)\n",
    "    builder.add_edge(\"tools\", \"call_model\")\n",
    "\n",
    "    return builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions for Implementation\n",
    "\n",
    "Your task is to configure the MCP server connections. The LangGraph implementation is already provided.\n",
    "\n",
    "### Your Task: Configure MCP Server Connections\n",
    "\n",
    "In the `connections` dictionary, you need to add two server configurations:\n",
    "\n",
    "1. **Filesystem Server Configuration**:\n",
    "2. **GitHub Server Configuration**:\n",
    "\n",
    "### Connection Configuration Format\n",
    "\n",
    "Each server configuration should follow this structure:\n",
    "```python\n",
    "\"server_name\": {\n",
    "    \"command\": \"command_to_run\",\n",
    "    \"args\": [\"list\", \"of\", \"arguments\"],\n",
    "    \"transport\": \"stdio\"\n",
    "}\n",
    "```\n",
    "\n",
    "Once you've configured the connections, the rest of the code will handle:\n",
    "- Creating the MultiServerMCPClient\n",
    "- Getting tools from both servers\n",
    "- Setting up the LangGraph with model and tool nodes\n",
    "- Managing the conversation flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_example() -> None:\n",
    "    \"\"\"\n",
    "    Run an example query against the graph.\n",
    "    \n",
    "    This function demonstrates the completed MCP integration by asking the agent\n",
    "    to fetch the latest issue from langchain-ai/langgraph and save a summary.\n",
    "    \"\"\"\n",
    "    # Set up the graph using your MCP configuration\n",
    "    graph = await setup_graph()\n",
    "    \n",
    "    # Define the prompt for the task\n",
    "    prompt = (\n",
    "        \"In the GitHub repository langchain-ai/langgraph (https://github.com/langchain-ai/langgraph), retrieve the most \"\n",
    "        \"recently created issue. Provide a concise summary of the issue and \"\n",
    "        \"append the summary to a file named 'issue_summary.txt' in the current \"\n",
    "        \"working directory. Confirm when the summary has been saved.\"\n",
    "    )\n",
    "    \n",
    "    # Invoke the graph with the prompt\n",
    "    result = await graph.ainvoke({\"messages\": [{\"role\": \"user\", \"content\": prompt}]})\n",
    "    \n",
    "    # Print the final assistant message\n",
    "    print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Your Implementation\n",
    "\n",
    "Once you've completed the functions above, run the cell below to test your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "await run_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Behavior\n",
    "\n",
    "When working correctly, your agent should:\n",
    "1. Use the GitHub server to fetch the latest issue from the langchain-ai/langgraph repository\n",
    "2. Process and summarize the issue content\n",
    "3. Use the filesystem server to save the summary to `issue_summary.txt`\n",
    "4. Confirm that the file has been saved successfully\n",
    "\n",
    "The final output should indicate that the task has been completed and the summary file has been created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
