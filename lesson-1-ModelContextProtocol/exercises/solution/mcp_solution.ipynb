{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCP Solution: Building a Multi-Server Agent\n",
    "\n",
    "This notebook contains the complete solution for building a LangGraph agent that integrates both filesystem and GitHub MCP servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install mcp, langgraph, and langchain if needed (uncomment when running locally)\n",
    "# !pip install langchain-mcp-adapters langgraph>=0.2.0 langchain-openai mcp\n",
    "\n",
    "import asyncio\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "import os\n",
    "## SET YOUR OPENAI_API_KEY HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Solution Implementation\n",
    "/Users/gp/Udacity/udacity-cd14639-intro-langchain/lesson-1-ModelContextProtocol/exercises/solution/langgraph_github_fs_demo.py\n",
    "This solution demonstrates how to:\n",
    "1. Configure MCP server connections for filesystem and GitHub servers\n",
    "2. Create a LangGraph agent that can use tools from both servers\n",
    "3. Orchestrate fetching GitHub issues and saving summaries to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def setup_graph() -> StateGraph:\n",
    "    \"\"\"\n",
    "    Create and return a compiled LangGraph that integrates MCP tools.\n",
    "    \n",
    "    This is the complete solution showing proper MCP server configuration.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # SOLUTION: Define MCP server connections\n",
    "    connections = {\n",
    "        \"filesystem\": {\n",
    "            \"command\": \"python\",\n",
    "            \"args\": [\"./filesystem_server.py\"],\n",
    "            \"transport\": \"stdio\",\n",
    "        },\n",
    "        \"github\": {\n",
    "            \"command\": \"python\",\n",
    "            \"args\": [\"./github_server.py\"],\n",
    "            \"transport\": \"stdio\",\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    # Create MCP client and get tools\n",
    "    client = MultiServerMCPClient(connections)\n",
    "    tools = await client.get_tools()\n",
    "    \n",
    "    # Initialize ChatOpenAI model and bind tools\n",
    "    model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "    model_with_tools = model.bind_tools(tools)\n",
    "    \n",
    "    # Create ToolNode for executing tool calls\n",
    "    tool_node = ToolNode(tools)\n",
    "    \n",
    "    # Define transition function for graph routing\n",
    "    def should_continue(state: MessagesState):\n",
    "        last = state[\"messages\"][-1]\n",
    "        if last.tool_calls:\n",
    "            return \"tools\"\n",
    "        return END\n",
    "    \n",
    "    # Define model invocation node\n",
    "    async def call_model(state: MessagesState):\n",
    "        messages = state[\"messages\"]\n",
    "        response = await model_with_tools.ainvoke(messages)\n",
    "        return {\"messages\": [response]}\n",
    "    \n",
    "    # Build the state graph\n",
    "    builder = StateGraph(MessagesState)\n",
    "    builder.add_node(\"call_model\", call_model)\n",
    "    builder.add_node(\"tools\", tool_node)\n",
    "    builder.add_edge(START, \"call_model\")\n",
    "    builder.add_conditional_edges(\"call_model\", should_continue)\n",
    "    builder.add_edge(\"tools\", \"call_model\")\n",
    "    \n",
    "    return builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Explanation\n",
    "\n",
    "### MCP Server Configuration\n",
    "\n",
    "The key part of this solution is the `connections` dictionary:\n",
    "\n",
    "```python\n",
    "connections = {\n",
    "    \"filesystem\": {\n",
    "        \"command\": \"python\",\n",
    "        \"args\": [\"./filesystem_server.py\"],\n",
    "        \"transport\": \"stdio\",\n",
    "    },\n",
    "    \"github\": {\n",
    "        \"command\": \"python\",\n",
    "        \"args\": [\"./github_server.py\"],\n",
    "        \"transport\": \"stdio\",\n",
    "    },\n",
    "}\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "- Each server has a unique name (\"filesystem\", \"github\")\n",
    "- `command`: The executable to run (\"python\" in both cases)\n",
    "- `args`: List of arguments passed to the command (the server script paths)\n",
    "- `transport`: Communication method (\"stdio\" for standard input/output)\n",
    "\n",
    "### How it Works\n",
    "1. `MultiServerMCPClient` uses these configurations to spawn the server processes\n",
    "2. It communicates with each server via stdio transport\n",
    "3. The client aggregates tools from both servers\n",
    "4. LangGraph agent can then use any tool from either server seamlessly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T07:31:22.361466Z",
     "start_time": "2025-09-19T07:31:22.359328Z"
    }
   },
   "outputs": [],
   "source": [
    "async def run_example() -> None:\n",
    "    \"\"\"\n",
    "    Run an example query against the graph.\n",
    "    \n",
    "    This demonstrates the complete working solution that fetches GitHub issues\n",
    "    and saves summaries using the configured MCP servers.\n",
    "    \"\"\"\n",
    "    # Set up the graph using the complete MCP configuration\n",
    "    graph = await setup_graph()\n",
    "    \n",
    "    # Define the prompt for the task\n",
    "    prompt = (\n",
    "        \"In the GitHub repository langchain-ai/langgraph (https://github.com/langchain-ai/langgraph), retrieve the most \"\n",
    "        \"recently created issue. Provide a concise summary of the issue and \"\n",
    "        \"append the summary to a file named 'issue_summary.txt' in the current \"\n",
    "        \"working directory. Confirm when the summary has been saved.\"\n",
    "    )\n",
    "    \n",
    "    # Invoke the graph with the prompt\n",
    "    result = await graph.ainvoke({\"messages\": [{\"role\": \"user\", \"content\": prompt}]})\n",
    "    \n",
    "    # Print the final assistant message\n",
    "    print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Complete Solution\n",
    "\n",
    "Run the cell below to test the complete implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T07:31:33.401134Z",
     "start_time": "2025-09-19T07:31:25.248885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The summary of the most recent issue has been successfully appended to 'issue_summary.txt'.\n"
     ]
    }
   ],
   "source": [
    "# Test the complete solution\n",
    "await run_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Behavior\n",
    "\n",
    "When running correctly, the solution will:\n",
    "\n",
    "1. **Connect to MCP Servers**: Spawn both filesystem and GitHub server processes\n",
    "2. **Fetch GitHub Issue**: Use GitHub server tools to retrieve the latest issue from langchain-ai/langgraph\n",
    "3. **Process Content**: Analyze and summarize the issue content\n",
    "4. **Save to File**: Use filesystem server tools to save the summary to `issue_summary.txt`\n",
    "5. **Confirm Success**: Report that the task has been completed\n",
    "\n",
    "The agent seamlessly orchestrates between the two MCP servers to complete the multi-step task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Learning Points\n",
    "\n",
    "This solution demonstrates several important MCP concepts:\n",
    "\n",
    "1. **Multi-Server Configuration**: How to connect to multiple MCP servers simultaneously\n",
    "2. **Tool Aggregation**: Tools from different servers are combined into a single tool set\n",
    "3. **Seamless Integration**: The agent doesn't need to know which server provides which tool\n",
    "4. **Flexible Architecture**: Easy to add more servers by extending the connections dictionary\n",
    "5. **Server Isolation**: Each server runs in its own process with defined boundaries\n",
    "\n",
    "The MCP architecture allows for modular, scalable AI applications that can leverage specialized capabilities from different sources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
